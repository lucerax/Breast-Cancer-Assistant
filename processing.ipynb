{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbJC0giF/Z5Kg200t723K3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIkxKqjTb5N",
        "colab_type": "text"
      },
      "source": [
        "**Loading the Data** \\\\\n",
        "First, we take care of the main necessary modules and load training and test data (formatted using preprocessing.py script)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKRd5WwTRoPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.applications import DenseNet201\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJBIUG9kTbYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Dataset_loader(DIR, RESIZE, sigmaX=10):\n",
        "    IMG = []\n",
        "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
        "    for IMAGE_NAME in tqdm(os.listdir(DIR)):\n",
        "        PATH = os.path.join(DIR,IMAGE_NAME)\n",
        "        _, ftype = os.path.splitext(PATH)\n",
        "        if ftype == \".png\":\n",
        "            img = read(PATH)\n",
        "            img = cv2.resize(img, (RESIZE,RESIZE))\n",
        "            IMG.append(np.array(img))\n",
        "    return IMG\n",
        "\n",
        "benign_train = np.array(Dataset_loader(os.path.join('bk_data', 'train', 'benign'), 224))\n",
        "malign_train = np.array(Dataset_loader(os.path.join('bk_data', 'train', 'malignant'), 224))\n",
        "benign_test = np.array(Dataset_loader(os.path.join('bk_data', 'test', 'benign'), 224))\n",
        "malign_test = np.array(Dataset_loader(os.path.join('bk_data', 'test', 'malignant'), 224))\n",
        "\n",
        "benign_train_label = np.zeros(len(benign_train))\n",
        "malign_train_label = np.ones(len(malign_train))\n",
        "benign_test_label = np.zeros(len(benign_test))\n",
        "malign_test_label = np.ones(len(malign_test))\n",
        "\n",
        "X_train = np.concatenate((benign_train, malign_train), axis = 0)\n",
        "Y_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)\n",
        "X_test = np.concatenate((benign_test, malign_test), axis = 0)\n",
        "Y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\n",
        "\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "Y_train = Y_train[s]\n",
        "\n",
        "s = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_test = X_test[s]\n",
        "Y_test = Y_test[s]\n",
        "\n",
        "Y_train = to_categorical(Y_train, num_classes= 2)\n",
        "Y_test = to_categorical(Y_test, num_classes= 2)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    X_train, Y_train,\n",
        "    test_size=0.2,\n",
        "    random_state=11\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ImZ_ciT7Dr",
        "colab_type": "text"
      },
      "source": [
        "**Build Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n9wfCZ2U8u5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import load_model\n",
        "from model import *\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "def cnn(img_shape=(224, 224, 3)):\n",
        "\n",
        "    classifier = Sequential()\n",
        "    #group A\n",
        "    #classifier.add(ZeroPadding2D(padding=(1, 1)))\n",
        "    classifier.add(Conv2D(filters = 32, kernel_size = (3,3), input_shape = img_shape))\n",
        "    classifier.add(Conv2D(filters = 32, kernel_size = (3,3)))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())\n",
        "    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "    #group B\n",
        "    classifier.add(ZeroPadding2D(padding=(1, 1)))\n",
        "    classifier.add(Conv2D(filters = 64, kernel_size = (3,3)))\n",
        "    classifier.add(Conv2D(filters = 64, kernel_size = (3,3)))\n",
        "    classifier.add(Activation('relu'))\n",
        "    classifier.add(BatchNormalization())\n",
        "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = 2))\n",
        "\n",
        "    #FCN\n",
        "    classifier.add(Flatten())\n",
        "    classifier.add(Dense(activation = 'relu', units = 512))\n",
        "    classifier.add(Dropout(0.5))\n",
        "\n",
        "    classifier.add(Dense(activation = 'relu', units = 512))\n",
        "    classifier.add(Dropout(0.5))\n",
        "\n",
        "    classifier.add(Dense(activation = 'sigmoid', units = 2))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "\n",
        "#model = cnn(img_shape = (32, 32, 3))\n",
        "#input_shape = (None, 32, 32, 3)\n",
        "#model.build(input_shape)\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "def build_model(backbone, lr=1e-4):\n",
        "    model = Sequential()\n",
        "    model.add(backbone)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(lr=lr),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "resnet = DenseNet201(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "model = build_model(resnet ,lr = 1e-4)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH71SnCOU9_M",
        "colab_type": "text"
      },
      "source": [
        "**Train Model** \\\\\n",
        "We use a generator with data augmentation as well as callbacks to save progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT-nla6tT5Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "\n",
        "train_generator = ImageDataGenerator(\n",
        "        zoom_range=2,  # set range for random zoom\n",
        "        rotation_range = 90,\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY37BJrOVQYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"dense.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        "    verbose = 1)\n",
        "learn_control = ReduceLROnPlateau(monitor='val_acc', patience=5,\n",
        "                                  verbose=1,factor=0.2, min_lr=1e-7)\n",
        "\n",
        "\n",
        "\n",
        "#train the model with new callback\n",
        "history = model.fit_generator(\n",
        "    train_generator.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[learn_control, checkpoint]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "model.save('my_model2.h5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rgSlaLhVVS_",
        "colab_type": "text"
      },
      "source": [
        "Checking GPU usage "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM1b2Ej-VXEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as tfback\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "print(\"tf.__version__ is\", tf.__version__)\n",
        "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
        "\n",
        "def _get_available_gpus():\n",
        "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
        "\n",
        "    # Returns\n",
        "        A list of available GPU devices.\n",
        "    \"\"\"\n",
        "    #global _LOCAL_DEVICES\n",
        "    if tfback._LOCAL_DEVICES is None:\n",
        "        devices = tf.config.list_logical_devices()\n",
        "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "\n",
        "tfback._get_available_gpus = _get_available_gpus\n",
        "tfback._get_available_gpus()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KUVIfxyVbrd",
        "colab_type": "text"
      },
      "source": [
        "To continue training, we load up the .h5 file that contains model architecture, weights etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkW5vgBNVown",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CONTINUATION OF TRAINING\n",
        "new_model = models.load_model('keras-flask-deploy-webapp/models/my_model2.h5')\n",
        "history = new_model.fit_generator(\n",
        "    train_generator.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[learn_control, checkpoint],\n",
        "    workers=100,\n",
        "    max_queue_size=100\n",
        "    )\n",
        "new_model.save('dense_model2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMVzf6yDWAAn",
        "colab_type": "text"
      },
      "source": [
        "**Prediction and Data Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvhXF1P8VtSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_path = os.path.join('test_images')\n",
        "single = np.array(Dataset_loader(single_path, 224))\n",
        "temp = new_model.predict(single)\n",
        "Y_pred = new_model.predict(X_test)\n",
        "model = models.load_model('keras-flask-deploy-webapp/models/my_model2.h5')\n",
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP6jURCcVyRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=55)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))\n",
        "\n",
        "cm_plot_label =['benign', 'malignant']\n",
        "plot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for Breast Cancer')\n",
        "\n",
        "precision = cm[1][1]/(cm[0][1] + cm[1][1])\n",
        "recall = cm[1][1]/(cm[1][1] + cm[1][0])\n",
        "specificity = cm[0][0]/(cm[0][0] + cm[1][0])\n",
        "accuracy = (cm[0][0] + cm[1][1])/np.sum(cm)\n",
        "f1 = (2* recall * precision)/(recall + precision)\n",
        "print(\"precision = \", precision)\n",
        "print(\"recall/sensitivity = \", recall)\n",
        "print(\"specificity = \", specificity)\n",
        "print(\"accuracy = \", accuracy)\n",
        "print (\"f1 = \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}